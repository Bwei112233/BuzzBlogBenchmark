{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Queue Length Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functionalities\n",
    "- Plot number of connections to each server and thresholds for queueing and packet dropping (max. number of threads and TCP buffer size, respectively).\n",
    "- Plot queue length of each server.\n",
    "\n",
    "## Input\n",
    "Log files are read from a directory in `../data`. This directory is assumed to have the following structure:\n",
    "```\n",
    "logs/\n",
    "  [node-1]/\n",
    "    *_service*.tar.gz\n",
    "    ...\n",
    "    apigateway*.tar.gz\n",
    "  ...\n",
    "  [node-n]/\n",
    "    *_service*.tar.gz\n",
    "    ...\n",
    "    apigateway*.tar.gz\n",
    "```\n",
    "`*_service*.tar.gz` and `apigateway*.tar.gz` tarballs contain RPC log files named `calls.log` and database query log files named `queries.log`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## GENERAL\n",
    "# Name of the directory in `../data`\n",
    "EXPERIMENT_DIRNAME = \"BuzzBlogBenchmark_2021-11-04-04-05-41\"\n",
    "\n",
    "########## CONNECTION\n",
    "# Window size\n",
    "WINDOW_IN_MS = 1\n",
    "\n",
    "########## THRIFT SERVERS\n",
    "THRIFT_SOMAXCONN = 128\n",
    "THRIFT_THREADS = 8\n",
    "\n",
    "########## POSTGRES SERVERS\n",
    "PG_SOMAXCONN = 128\n",
    "PG_MAX_CONNECTIONS = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries.\n",
    "%matplotlib inline\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import tarfile\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Constants\n",
    "COMPONENT_TARBALL_PATTERNS = [\n",
    "    r\"^apigateway.*\\.tar\\.gz$\",\n",
    "    r\"^.+_service.*\\.tar\\.gz$\",\n",
    "]\n",
    "RPC_LOG_PATTERN = r\"^\\[(.+)\\] pid=(.+) tid=(.+) request_id=(.+) server=(.+) function=(.+) latency=(.+)$\"\n",
    "QUERY_LOG_PATTERN = r\"^\\[(.+)\\] pid=(.+) tid=(.+) request_id=(.+) dbname=(.+) latency=(.+) query=\\\"(.+)\\\"$\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RPC Log Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse logs\n",
    "rpc = {\"node_name\": [], \"timestamp\": [], \"request_id\": [], \"server\": [], \"function\": [], \"latency\": []}\n",
    "node_names = os.listdir(os.path.join(os.pardir, \"data\", EXPERIMENT_DIRNAME, \"logs\"))\n",
    "for node_name in node_names:\n",
    "  for tarball_name in os.listdir(os.path.join(os.pardir, \"data\", EXPERIMENT_DIRNAME, \"logs\", node_name)):\n",
    "    if sum([1 if re.match(tarball_pattern, tarball_name) else 0 for tarball_pattern in COMPONENT_TARBALL_PATTERNS]):\n",
    "      tarball_path = os.path.join(os.pardir, \"data\", EXPERIMENT_DIRNAME, \"logs\", node_name, tarball_name)\n",
    "      with tarfile.open(tarball_path, \"r:gz\") as tar:\n",
    "        for filename in tar.getnames():\n",
    "          if filename.endswith(\"calls.log\"):\n",
    "            with tar.extractfile(filename) as log_file:\n",
    "              for row in log_file:\n",
    "                rpc_log_match = re.match(RPC_LOG_PATTERN, row.decode(\"utf-8\"))\n",
    "                if rpc_log_match:\n",
    "                  timestamp, pid, tid, request_id, server, function, latency = rpc_log_match.groups()\n",
    "                  rpc[\"node_name\"].append(node_name)\n",
    "                  rpc[\"timestamp\"].append(datetime.datetime.strptime(timestamp[:-3], \"%H:%M:%S.%f\"))\n",
    "                  rpc[\"request_id\"].append(request_id)\n",
    "                  rpc[\"server\"].append(server)\n",
    "                  rpc[\"function\"].append(function)\n",
    "                  rpc[\"latency\"].append(float(latency) * 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build data frame\n",
    "rpc = pd.DataFrame.from_dict(rpc)\n",
    "\n",
    "# Get values\n",
    "servers = sorted(rpc[\"server\"].unique())\n",
    "min_timestamp = rpc[\"timestamp\"].values.min()\n",
    "\n",
    "# (Re) Build columns\n",
    "rpc[\"timestamp\"] = rpc.apply(lambda r: (r[\"timestamp\"] - min_timestamp).total_seconds(), axis=1)\n",
    "rpc[\"window\"] = rpc.apply(lambda r: range(int(r[\"timestamp\"] * 1000) // WINDOW_IN_MS,\n",
    "    int((r[\"timestamp\"] + r[\"latency\"] / 1000) * 1000) // WINDOW_IN_MS + 1), axis=1)\n",
    "rpc = rpc.explode(\"window\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Microservices Queue Length Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## LOCAL CONFIG\n",
    "# Minimum time (in seconds)\n",
    "MIN_TIME = None\n",
    "# Maximum time (in seconds)\n",
    "MAX_TIME = None\n",
    "\n",
    "# Plot\n",
    "fig = plt.figure(figsize=(24, len(servers) * 12))\n",
    "for (i, server) in enumerate(servers):\n",
    "    df = rpc[(rpc[\"server\"] == server)]\n",
    "    if MIN_TIME:\n",
    "        df = df[(df[\"timestamp\"] >= MIN_TIME)]\n",
    "    if MAX_TIME:\n",
    "        df = df[(df[\"timestamp\"] <= MAX_TIME)]\n",
    "    df = df.groupby([\"window\"])[\"window\"].count()\n",
    "    df = df.reindex(range(int(df.index.min()), int(df.index.max()) + 1), fill_value=0)\n",
    "    ax = fig.add_subplot(len(servers), 1, i + 1)\n",
    "    ax.grid(alpha=0.75)\n",
    "    ax.set_xlim((df.index.min(), df.index.max()))\n",
    "    ax.set_ylim((0, df.values.max()))\n",
    "    ax.axhline(y=THRIFT_THREADS, ls=\"--\", color=\"blue\", linewidth=5)\n",
    "    ax.axhline(y=THRIFT_THREADS + THRIFT_SOMAXCONN, ls=\"--\", color=\"red\", linewidth=5)\n",
    "    df.plot(ax=ax, kind=\"line\",\n",
    "        title=\"%s - Number of connections (%s-millisecond window)\" % (server, WINDOW_IN_MS),\n",
    "        xlabel=\"Time (s)\", ylabel=\"Connections (count)\", color=\"black\", grid=True,\n",
    "        xticks=range(int(df.index.min()), int(df.index.max()) + 1, 60 * (1000 // WINDOW_IN_MS)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## LOCAL CONFIG\n",
    "# Minimum time (in seconds)\n",
    "MIN_TIME = None\n",
    "# Maximum time (in seconds)\n",
    "MAX_TIME = None\n",
    "\n",
    "# Plot\n",
    "fig = plt.figure(figsize=(24, len(servers) * 12))\n",
    "for (i, server) in enumerate(servers):\n",
    "    df = rpc[(rpc[\"server\"] == server)]\n",
    "    if MIN_TIME:\n",
    "        df = df[(df[\"timestamp\"] >= MIN_TIME)]\n",
    "    if MAX_TIME:\n",
    "        df = df[(df[\"timestamp\"] <= MAX_TIME)]\n",
    "    df = df.groupby([\"window\"])[\"window\"].count()\n",
    "    df = df.apply(lambda r: max(r - THRIFT_THREADS, 0))\n",
    "    df = df.reindex(range(int(df.index.min()), int(df.index.max()) + 1), fill_value=0)\n",
    "    ax = fig.add_subplot(len(servers), 1, i + 1)\n",
    "    ax.grid(alpha=0.75)\n",
    "    ax.set_xlim((df.index.min(), df.index.max()))\n",
    "    ax.set_ylim((0, df.values.max()))\n",
    "    ax.axhline(y=THRIFT_SOMAXCONN, ls=\"--\", color=\"red\", linewidth=5)\n",
    "    df.plot(ax=ax, kind=\"line\",\n",
    "        title=\"%s - Queue Length (%s-millisecond window)\" % (server, WINDOW_IN_MS),\n",
    "        xlabel=\"Time (s)\", ylabel=\"Connections (count)\", color=\"black\", grid=True,\n",
    "        xticks=range(int(df.index.min()), int(df.index.max()) + 1, 60 * (1000 // WINDOW_IN_MS)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Log Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse logs\n",
    "query = {\"node_name\": [], \"timestamp\": [], \"request_id\": [], \"dbname\": [], \"type\": [], \"latency\": []}\n",
    "node_names = os.listdir(os.path.join(os.pardir, \"data\", EXPERIMENT_DIRNAME, \"logs\"))\n",
    "for node_name in node_names:\n",
    "  for tarball_name in os.listdir(os.path.join(os.pardir, \"data\", EXPERIMENT_DIRNAME, \"logs\", node_name)):\n",
    "    if sum([1 if re.match(tarball_pattern, tarball_name) else 0 for tarball_pattern in COMPONENT_TARBALL_PATTERNS]):\n",
    "      tarball_path = os.path.join(os.pardir, \"data\", EXPERIMENT_DIRNAME, \"logs\", node_name, tarball_name)\n",
    "      with tarfile.open(tarball_path, \"r:gz\") as tar:\n",
    "        for filename in tar.getnames():\n",
    "          if filename.endswith(\"queries.log\"):\n",
    "            with tar.extractfile(filename) as log_file:\n",
    "              for row in log_file:\n",
    "                query_log_match = re.match(QUERY_LOG_PATTERN, row.decode(\"utf-8\"))\n",
    "                if query_log_match:\n",
    "                  timestamp, pid, tid, request_id, dbname, latency, query_str = query_log_match.groups()\n",
    "                  query[\"node_name\"].append(node_name)\n",
    "                  query[\"timestamp\"].append(datetime.datetime.strptime(timestamp[:-3], \"%H:%M:%S.%f\"))\n",
    "                  query[\"request_id\"].append(request_id)\n",
    "                  query[\"dbname\"].append(dbname)\n",
    "                  query[\"type\"].append(query_str.strip().split()[0].upper())\n",
    "                  query[\"latency\"].append(float(latency) * 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build data frame\n",
    "query = pd.DataFrame.from_dict(query)\n",
    "\n",
    "# Get values\n",
    "dbnames = sorted(query[\"dbname\"].unique())\n",
    "min_timestamp = query[\"timestamp\"].values.min()\n",
    "\n",
    "# (Re) Build columns\n",
    "query[\"timestamp\"] = query.apply(lambda q: (q[\"timestamp\"] - min_timestamp).total_seconds(), axis=1)\n",
    "query[\"window\"] = query.apply(lambda q: range(int(q[\"timestamp\"] * 1000) // WINDOW_IN_MS,\n",
    "    int((q[\"timestamp\"] + q[\"latency\"] / 1000) * 1000) // WINDOW_IN_MS + 1), axis=1)\n",
    "query = query.explode(\"window\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Databases Queue Length Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## LOCAL CONFIG\n",
    "# Minimum time (in seconds)\n",
    "MIN_TIME = None\n",
    "# Maximum time (in seconds)\n",
    "MAX_TIME = None\n",
    "\n",
    "# Plot\n",
    "fig = plt.figure(figsize=(24, len(servers) * 12))\n",
    "for (i, dbname) in enumerate(dbnames):\n",
    "    df = query[(query[\"dbname\"] == dbname)]\n",
    "    if MIN_TIME:\n",
    "        df = df[(df[\"timestamp\"] >= MIN_TIME)]\n",
    "    if MAX_TIME:\n",
    "        df = df[(df[\"timestamp\"] <= MAX_TIME)]\n",
    "    df = df.groupby([\"window\"])[\"window\"].count()\n",
    "    df = df.reindex(range(int(df.index.min()), int(df.index.max()) + 1), fill_value=0)\n",
    "    ax = fig.add_subplot(len(servers), 1, i + 1)\n",
    "    ax.grid(alpha=0.75)\n",
    "    ax.set_xlim((df.index.min(), df.index.max()))\n",
    "    ax.set_ylim((0, df.values.max()))\n",
    "    ax.axhline(y=PG_MAX_CONNECTIONS, ls=\"--\", color=\"blue\", linewidth=5)\n",
    "    ax.axhline(y=PG_MAX_CONNECTIONS + PG_SOMAXCONN, ls=\"--\", color=\"red\", linewidth=5)\n",
    "    df.plot(ax=ax, kind=\"line\",\n",
    "        title=\"%s - Number of connections (%s-millisecond window)\" % (dbname, WINDOW_IN_MS),\n",
    "        xlabel=\"Time (s)\", ylabel=\"Connections (count)\", color=\"black\", grid=True,\n",
    "        xticks=range(int(df.index.min()), int(df.index.max()) + 1, 60 * (1000 // WINDOW_IN_MS)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## LOCAL CONFIG\n",
    "# Minimum time (in seconds)\n",
    "MIN_TIME = None\n",
    "# Maximum time (in seconds)\n",
    "MAX_TIME = None\n",
    "\n",
    "# Plot\n",
    "fig = plt.figure(figsize=(24, len(servers) * 12))\n",
    "for (i, dbname) in enumerate(dbnames):\n",
    "    df = query[(query[\"dbname\"] == dbname)]\n",
    "    if MIN_TIME:\n",
    "        df = df[(df[\"timestamp\"] >= MIN_TIME)]\n",
    "    if MAX_TIME:\n",
    "        df = df[(df[\"timestamp\"] <= MAX_TIME)]\n",
    "    df = df.groupby([\"window\"])[\"window\"].count()\n",
    "    df = df.apply(lambda r: max(r - PG_MAX_CONNECTIONS, 0))\n",
    "    df = df.reindex(range(int(df.index.min()), int(df.index.max()) + 1), fill_value=0)\n",
    "    ax = fig.add_subplot(len(servers), 1, i + 1)\n",
    "    ax.grid(alpha=0.75)\n",
    "    ax.set_xlim((df.index.min(), df.index.max()))\n",
    "    ax.set_ylim((0, df.values.max()))\n",
    "    ax.axhline(y=PG_SOMAXCONN, ls=\"--\", color=\"red\", linewidth=5)\n",
    "    df.plot(ax=ax, kind=\"line\",\n",
    "        title=\"%s - Queue Length (%s-millisecond window)\" % (dbname, WINDOW_IN_MS),\n",
    "        xlabel=\"Time (s)\", ylabel=\"Connections (count)\", color=\"black\", grid=True,\n",
    "        xticks=range(int(df.index.min()), int(df.index.max()) + 1, 60 * (1000 // WINDOW_IN_MS)))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
